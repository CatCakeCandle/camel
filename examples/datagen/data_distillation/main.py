# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from datasets import load_dataset
import pandas as pd
import json
from datetime import datetime
import os
from getpass import getpass
from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType
from camel.configs import DeepSeekConfig

def setup_ai():
    r"""Initialize and configure AI models and agents for solving and checking math problems.

    Returns:
        tuple: A tuple containing two ChatAgent instances:
            - solver_agent: An agent for solving math problems.
            - checker_agent: An agent for checking the consistency of answers.
    """
    # Create models
    generator_model = ModelFactory.create(
        model_platform=ModelPlatformType.DEEPSEEK,
        model_type=ModelType.DEEPSEEK_REASONER,
        model_config_dict=DeepSeekConfig().as_dict()
    )
    checker_model=ModelFactory.create(
        model_platform=ModelPlatformType.DEEPSEEK,
        model_type=ModelType.DEEPSEEK_CHAT,
        model_config_dict=DeepSeekConfig().as_dict()
    )
    # Create solver agent
    solver_agent = ChatAgent(
        system_message="You are an AI assistant proficient in mathematics, skilled at solving various math problems. Please provide detailed problem-solving steps and clearly mark the final answer with 'Answer:' at the end.",
        model=generator_model,
        message_window_size=10
    )
    
    # Create checker agent
    checker_agent = ChatAgent(
        system_message="""You are a rigorous mathematics expert responsible for judging whether answers are equivalent.
Judgment criteria:
1. Even if the expressions are different, they are considered consistent as long as the mathematical meaning is the same.
2. Different mathematical representations (such as fractions, decimals, percentages, etc.) need to be considered.
3. Mathematical equivalence needs to be considered (e.g., 1/2 equals 0.5).
Please only reply with "consistent" or "inconsistent". If uncertain, reply with "inconsistent".""",
        model=checker_model,
        message_window_size=10
    )
    
    return solver_agent, checker_agent

def generate_ai_response(problem, solver_agent):
    r"""Generate a response to a math problem using the solver agent.

    Args:
        problem (str): The math problem to be solved.
        solver_agent (ChatAgent): The AI agent responsible for solving the problem.

    Returns:
        str: The AI-generated response to the math problem.
    """
    response = solver_agent.step(f"Please solve the following math problem:\n{problem}")
    solver_agent.reset()
    return response.msgs[0].content

def check_answer_consistency(ai_answer, standard_answer, checker_agent):
    r"""Check if the AI-generated answer is consistent with the standard answer.

    Args:
        ai_answer (str): The answer generated by the AI.
        standard_answer (str): The standard or correct answer.
        checker_agent (ChatAgent): The AI agent responsible for checking consistency.

    Returns:
        bool: True if the answers are consistent, False otherwise.
    """
    prompt = f"""Please judge whether the following two answers are mathematically equivalent:

Standard answer: {standard_answer}
AI response: {ai_answer}"""
    
    response = checker_agent.step(prompt)
    result = response.msgs[0].content.strip()
    
    return result == "consistent"

def evaluate_responses(dataset_path="problems_answers.csv", output_path="evaluation_results.json"):
    r"""Evaluate the consistency of AI-generated responses with standard answers.

    Args:
        dataset_path (str, optional): Path to the CSV file containing problems and 
            standard answers. Defaults to "problems_answers.csv".
        output_path (str, optional): Path to save the evaluation results in JSON 
            format. Defaults to "evaluation_results.json".

    Returns:
        None: The function saves the evaluation results to a JSON file and prints 
            the accuracy and other metrics to the console.
    """
    # Set up AI
    solver_agent, checker_agent = setup_ai()
    
    # Load dataset
    df = pd.read_csv(dataset_path)
    
    results = []
    total = len(df)
    correct = 0
    
    print(f"\nStarting evaluation of {total} problems...")
    
    for idx, row in df.iterrows():
        problem = row['problem']
        standard_answer = row['answer']
        
        try:
            # Get AI response
            ai_response = generate_ai_response(problem, solver_agent)
            
            # Use AI to judge whether the answer is consistent
            is_correct = check_answer_consistency(ai_response, standard_answer, checker_agent)
            
            if is_correct:
                correct += 1
            
            # Record result
            result = {
                'problem': problem,
                'standard_answer': standard_answer,
                'ai_response': ai_response,
                'is_correct': is_correct
            }
            
        except Exception as e:
            print(f"Error processing problem {idx + 1}: {str(e)}")
            result = {
                'problem': problem,
                'standard_answer': standard_answer,
                'ai_response': "Error: Processing failed",
                'is_correct': False,
                'error': str(e)
            }
        
        results.append(result)
        
        # Show progress
        if (idx + 1) % 5 == 0:  # Show progress every 5 problems
            print(f"Processed: {idx + 1}/{total} ({(idx + 1)/total*100:.1f}%)")
            print(f"Current accuracy: {(correct/(idx+1)*100):.1f}%")
    
    # Calculate accuracy
    accuracy = correct / total
    
    # Save results
    output = {
        'timestamp': datetime.now().isoformat(),
        'total_questions': total,
        'correct_answers': correct,
        'accuracy': accuracy,
        'results': results
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nEvaluation completed!")
    print(f"Total problems: {total}")
    print(f"Correct answers: {correct}")
    print(f"Accuracy: {accuracy:.2%}")
    print(f"Detailed results saved to: {output_path}")

if __name__ == "__main__":
    # First, ensure the dataset file exists
    if not os.path.exists("problems_answers.csv"):
        print("Error: Dataset file problems_answers.csv not found")
        print("Please run loaddata.py to generate the dataset file first")
        exit(1)
    
    # Run evaluation
    evaluate_responses()