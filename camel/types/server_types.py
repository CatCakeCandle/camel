# =========== Copyright 2023 @ CAMEL-AI.org. All Rights Reserved. ===========
# Licensed under the Apache License, Version 2.0 (the “License”);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an “AS IS” BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =========== Copyright 2023 @ CAMEL-AI.org. All Rights Reserved. ===========

from __future__ import annotations

from typing import Dict, List, Literal, Optional, Union

import torch
from openai.types.chat.chat_completion import (
    ChatCompletion,
)
from openai.types.chat.chat_completion_message_param import (
    ChatCompletionMessageParam,
)
from pydantic import BaseModel

DEFAULT_HF_DTYPE = torch.float16


class ListModelResponse(BaseModel):
    alias: str
    r"""
    Alias of registered model. Default to the model basename
    e.g. meta-llama/Llama-2-7b-chat-hf -> Llama-2-7b-chat-hf
    """
    basename: str
    r"""
    Base name of the model.. Default to the model basename
    e.g. meta-llama/Llama-2-7b-chat-hf -> Llama-2-7b-chat-hf
    """
    serving_engine: Literal["Huggingface", "VLLM"]
    r"""
    Model launching framework, either "Huggingface" or "VLLM"
    """


class ChatCompletionRequest(BaseModel):
    r"""Base class for completion create parameters.
    Not usable fields for open-source models sare
    commented out.
    # ref: https://platform.openai.com/docs/api-reference/chat/create
    """

    messages: List[ChatCompletionMessageParam]
    r"""
    A list of openai-style user-assistant alternating messages.
    """

    model: str
    r"""
    Model to use.
    """

    # frequency_penalty: Optional[float] = 0
    r"""
    Number between -2.0 and 2.0. Positive values penalize new tokens based
    on their existing frequency in the text so far, decreasing the model's
    likelihood to repeat the same line verbatim.
    """

    logit_bias: Optional[Dict] = None
    r"""
    Modify the likelihood of specified tokens appearing in the completion.
    
    Accepts a JSON object that maps tokens (specified by their token ID in the 
    tokenizer) to an associated bias value from -100 to 100. Mathematically, 
    the bias is added to the logits generated by the model prior to sampling. 
    The exact effect will vary per model, but values between -1 and 1 should 
    decrease or increase likelihood of selection; values like -100 or 100 
    should result in a ban or exclusive selection of the relevant token.
    """

    # TODO: fix -inf output score from HF model
    # logprobs: Optional[bool] = False
    r""""
    Whether to return log probabilities of the output tokens or not. If true, 
    returns the log probabilities of each output token returned in the 
    `content` of `message`. This option is currently not available on the 
    `gpt-4-vision-preview` model.
    """

    # top_logprobs: Optional[int] = None  # int between 0 and 5
    r"""
    An integer between 0 and 5 specifying the number of most likely tokens 
    to return at each token position, each with an associated log 
    probability. `logprobs` must be set to `true` if this parameter is used.
    """

    max_tokens: Optional[int] = None
    r"""
    The maximum number of tokens that can be generated in the chat completion.
    The total length of input tokens and generated tokens is limited by the 
    model's context length. Example Python code for counting tokens.
    """
    n: Optional[int] = 1
    r"""
    How many chat completion choices to generate for each input message. 
    Note that you will be charged based on the number of generated 
    tokens across all of the choices. Keep `n` as `1` to minimize costs.
    """
    # presence_penalty: Optional[float] = 0  # from -2.0 to 2.0
    r"""
    Number between -2.0 and 2.0. Positive values penalize new tokens 
    based on whether they appear in the text so far, increasing 
    the model's likelihood to talk about new topics.
    """
    # response_format: None
    # seed: None
    stop: Optional[Union[str, List[str]]] = None
    stream: Optional[bool] = False
    temperature: Optional[float] = 0.2  # from 0 to 2.0
    top_p: Optional[float] = 1.0  # from 0 to 1.0
    # tools: None
    # tool_choice: None
    # user:


ChatCompletionResponse = ChatCompletion


class HFModelLoadingParam(BaseModel):
    class Config:
        # pydantic unable to generate schema for torch.dtype
        arbitrary_types_allowed = True

    low_cpu_mem_usage: bool = True
    r"""
    Tries to not use more than 1x model size in CPU memory (including peak 
    memory) while loading the model. When passing a device_map, 
    low_cpu_mem_usage is automatically set to True
    #ref: https://huggingface.co/docs/transformers/main_classes/model
    """
    trust_remote_code: bool = True
    r"""
    Whether or not to allow for custom models defined on the Hub in their own 
    modeling files. This option should only be set to True for repositories you
    trust and in which you have read the code, as it will execute code present 
    on the Hub on your local machine.

    Need to be set to 'True' if the model hasn't been added to 
    huggingface/transformers yet. (e.g. internlm/internlm-chat-7b as Feb 2024)
    """
    device_map: Union[str, Dict[str, Union[int, str, torch.device]]] = "cuda"
    r"""
    A map that specifies where each submodule should go. It doesn't need to be 
    refined to each parameter/buffer name, once a given module name is inside, 
    every submodule of it will be sent to the same device. If we only pass the 
    device (e.g., "cpu", "cuda:1", "mps", or a GPU ordinal rank like 1) 
    on which the model will be allocated, the device map will map the entire 
    model to this device. Passing device_map = 0 means put the whole model on 
    GPU 0.
    """
    torch_dtype: torch.dtype = DEFAULT_HF_DTYPE
    r"""
    data type of the model
    """


class ModelRegistrationRequest(BaseModel):
    model: str
    r"""
    Can be either:
        - A string, the *model id* of a predefined model hosted inside a repo 
          on huggingface.co.
        - A path to a *directory* containing model weights saved using
          [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
    """
    alias: Optional[str] = None
    r"""
    Customized model nickname, used to identify model in the LLM manager
    """
    vllm: bool = False
    r"""
    Enable vllm framework
    """
    hf_param: HFModelLoadingParam = HFModelLoadingParam()


class ModelRemovalRequest(BaseModel):
    alias: str
    r"""
    Model to remove from the LLM manager
    """
